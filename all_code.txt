"""
Gridworld class and util
"""

import random
import matplotlib.pyplot as plt
import numpy as np


class Gridworld:
    """
    Gridworld environment
    """
    index_to_direction = [
        [0, -1],
        [0, 1],
        [-1, 0],
        [1, 0]]

    def __init__(self, n, start_x, start_y, end_states, random_start):
        """
        Parameters:
        n - Side of grid
        start_x - start on non random reset
        start_y - start on non random reset
        end_states - List of end states
        random_start - bool indicating should start randomly on reset
        """
        self.start_x = start_x
        self.start_y = start_y
        self.x = self.start_x
        self.y = self.start_y
        self.n = n
        self.end_states = end_states
        self.random_start = random_start

    def is_valid(self, x, y):
        """
        Checks if given (x,y) is valid
        """
        if x < 0 or x >= self.n or y < 0 or y >= self.n:
            return False
        return True

    def reset(self):
        """
        Reset env
        """
        if self.random_start:
            self.x = random.randint(0, self.n-1)
            self.y = random.randint(0, self.n-1)
        else:
            self.x = self.start_x
            self.y = self.start_y
        return (self.x, self.y)

    def get_actions(self):
        """
        Get all possible actions at current (x,y)
        """
        actions = []
        for i in range(len(self.index_to_direction)):
            if self.is_valid(self.x + self.index_to_direction[i][0], self.y + self.index_to_direction[i][1]):
                actions.append(i)
        return actions

    def get_all_actions(self):
        """
        get all possible actions in env irrespective of state
        """
        return [i for i in range(len(self.index_to_direction))]

    def get_actions_at_state(self, state):
        """
            get valid actions at state
        """
        actions = []
        for i in range(len(self.index_to_direction)):
            if self.is_valid(state[0] + self.index_to_direction[i][0], state[1] + self.index_to_direction[i][1]):
                actions.append(i)
        return actions

    def get_random_action_at_state(self, state):
        """
            get some random action at state
        """
        return random.choice(self.get_actions_at_state(state))

    def get_random_action(self):
        """
            get random action at current x,y
        """
        return random.choice(self.get_actions())

    def pos_after_step(self, pos, action):
        x = pos[0] + self.index_to_direction[action][0]
        y = pos[1] + self.index_to_direction[action][1]
        return (x, y)

    def step(self, action):
        """
            take a step from action
        """
        self.x = self.x + self.index_to_direction[action][0]
        self.y = self.y + self.index_to_direction[action][1]

        reward = -1
        done = False
        if (self.x, self.y) in self.end_states:
            reward = 0
            done = True

        return (self.x, self.y), reward, done, None
import csv
import numpy as np
import matplotlib.pyplot as plt

csv_log_file = None
csv_log_writer = None
fieldnames = ['episode', 'reward']


def get_filename_without_extension(model_name, params):
    file_name = "logs/" + model_name
    for k, v in params.items():
        file_name = file_name + "_" + str(k) + "=" + str(v)
    return file_name


def save_heat_map_mc(model, params, state_action, n):
    g = []
    for i in range(n):
        t = []
        for j in range(n):
            t.append(-9999.0)
        g.append(t)

    for k, v in state_action.items():
        g[k[0][0]][k[0][1]] = max(g[k[0][0]][k[0][1]], v[0])

    for i in range(n):
        for j in range(n):
            if(g[i][j] == -9999.0):
                g[i][j] = 0

    a = np.array(g)
    out_file_name = get_filename_without_extension(model, params) + ".png"
    print("Saving head map for", out_file_name)
    plt.imshow(a, cmap='hot', interpolation='nearest')
    plt.savefig(out_file_name)


def create_file(model_name, params):
    global csv_log_writer, csv_log_file
    csv_log_file = open(get_filename_without_extension(
        model_name, params) + ".csv", mode='w')
    csv_log_writer = csv.writer(csv_log_file, delimiter=',')
    csv_log_writer.writerow(fieldnames)


def write_entry(episode, reward):
    global csv_log_writer, csv_log_file
    csv_log_writer.writerow([episode, reward])


def save_file():
    global csv_log_writer, csv_log_file
    csv_log_file.close()


def save_heat_map_td(model, params, state_action, n):
    g = []
    for i in range(n):
        t = []
        for j in range(n):
            t.append(-9999.0)
        g.append(t)

    for i in range(n):
        for j in range(n):
            for k, v in state_action.items():
                if k[0] == (i, j):
                    g[i][j] = max(g[i][j], v)

    for i in range(n):
        for j in range(n):
            if(g[i][j] == -9999.0):
                g[i][j] = 0

    a = np.array(g)
    out_file_name = get_filename_without_extension(model, params) + ".png"
    print("Saving head map for", out_file_name)
    plt.imshow(a, cmap='hot', interpolation='nearest')
    plt.savefig(out_file_name)
import TD0_cartpole as td0cp
import TD0_gridworld as td0gw
import monte_carlo_cartpole as mccp
import monte_carlo_gridworld as mcgw
import TDN_gridworld as tdngw
import TDN_cartpole as tdncp

print("Running all experiments")

print("Running monte carlo on gridworld")
mcgw.train()

print("Running monte carlo on cartpole")
mccp.train()

print("Running TD0 on gridworld")
td0gw.train()

print("Running TD0 on cartpole")
td0cp.train()

print("Running TDN on gridworld")
tdngw.train()

print("Running TDN on cartpole")
tdncp.train()
import pickle

#  t = {}
#  t[((1, 2), 0)] = [4, 5]


def save_model(model, file_name):
    """
    Save model
    """
    with open(file_name, "wb") as pickle_out:
        pickle.dump(model, pickle_out)


def load_model(file_name):
    """
    Load model
    """
    with open(file_name, "rb") as pickle_in:
        model = pickle.load(pickle_in)
        return model
import random
import gym
from log_util import *

"""
class that performs monte carlo
"""


class MonteCarloCartpole:
    """
    monte carlo for RL
    """

    def __init__(self,  env):
        self.state_action = {}
        self.env = env
        self.scale = 12
        self.explore = 13

    def get_discrete_obs(self, observation):
        t = (observation * self.scale).astype(int)
        return tuple(t)

    def get_action(self, observation):
        t = random.random()

        action = self.env.action_space.sample()
        best_score = 0
        for k, v in self.state_action.items():
            if k[0] == observation and v[0] > best_score:
                action = k[1]
                best_score = v[0]

        if best_score > 0 and best_score/self.explore > t:
            return action
        else:
            return self.env.action_space.sample()

    def get_episode(self, ep_len):
        # (state,action,reward)
        episode = []
        observation = self.env.reset()
        observation = self.get_discrete_obs(observation)

        for _ in range(ep_len):
            action = self.get_action(observation)
            new_observation, reward, done, _ = self.env.step(action)
            new_observation = self.get_discrete_obs(new_observation)

            episode.append([observation, action, reward])
            observation = new_observation

            if done:
                break

        return episode

    def eval_episode(self, episode):
        episode.reverse()
        reward_so_far = 0

        for state, action, reward in episode:
            #         print(state,action,reward)
            reward_so_far = reward_so_far + reward

            if (state, action) in self.state_action:
                value, count = self.state_action[(state, action)]
                new_val = (value * count + reward_so_far) / (count + 1)
                self.state_action[(state, action)] = [new_val, count + 1]

            else:
                self.state_action[(state, action)] = [reward_so_far, 1]

    def train(self, no_of_eps, ep_len):
        avg_len = 0
        max_len = 100
        for i in range(1, no_of_eps):
            ep = None
            ep = self.get_episode(ep_len)
            self.eval_episode(ep)
            avg_len = avg_len + len(ep)
            write_entry(i, len(ep))

            if i % max_len == 0:
                print("Ep ", i, "Avg is", avg_len/max_len)
                avg_len = 0

    def run(self, no_of_runs):
        for _ in range(no_of_runs):
            observation = self.env.reset()
            while True:
                action = self.get_action(observation)
                new_observation, _, done, _ = self.env.step(action)
                new_observation = self.get_discrete_obs(new_observation)
                self.env.render()

                observation = new_observation

                if done:
                    break


def train():
    create_file("mc_cartpole", {})
    env = gym.make('CartPole-v0')
    mc = MonteCarloCartpole(env)
    mc.train(1000, 502)

    save_file()
import random
import gym
from gridworld import *
from monte_carlo_util import *
from log_util import *

"""
class that performs monte carlo
"""


class MonteCarloGridworld:
    """
    monte carlo for RL
    """

    def __init__(self,  env, exploration):
        self.state_action = {}
        self.env = env
        self.exploration = exploration

    def get_action(self, observation):
        t = random.random()

        if t < self.exploration:
            return self.env.get_random_action()
        else:
            action = self.env.get_random_action()
            best_score = 0
            for k, v in self.state_action.items():
                if k[0] == observation and v[0] > best_score:
                    action = k[1]
                    best_score = v[0]
            return action

    def get_episode(self, no_of_steps):
        # (state,action,reward)
        episode = []
        observation = self.env.reset()

        if observation in self.env.end_states:
            return episode

        for _ in range(no_of_steps):
            action = self.get_action(observation)
            new_observation, reward, done, _ = self.env.step(action)

            episode.append([observation, action, reward])
            observation = new_observation
            if done:
                break
        return episode

    def eval_episode(self, episode):
        episode.reverse()
        reward_so_far = 0

        for state, action, reward in episode:
            reward_so_far = reward_so_far + reward
            if (state, action) in self.state_action:
                value, count = self.state_action[(state, action)]
                new_val = (value * count + reward_so_far) / (count + 1)
                self.state_action[(state, action)] = [new_val, count + 1]

            else:
                self.state_action[(state, action)] = [reward_so_far, 1]

    def train(self, no_of_eps, episode_length):
        for i in range(1, no_of_eps):
            ep = self.get_episode(episode_length)
    #         print(ep)
            self.eval_episode(ep)
            self.exploration = self.exploration * 0.9999


params = {
    'n': 15,
    'learning_rate': 0.7,
    'end_states': {(1, 7), (9, 12)}
}


def train():
    global params
    env = Gridworld(15, 3, 3, params['end_states'], True)
    mc = MonteCarloGridworld(env, params['learning_rate'])
    mc.train(1000, 100)

    save_heat_map_mc("mc_gridworld", params, mc.state_action, params['n'])
    #  print_heat_map(mc.state_action, env.n)
import matplotlib.pyplot as plt
import numpy as np


def print_state_action(state_action, n):
    g = []
    for i in range(n):
        t = []
        for j in range(n):
            t.append(-9999.0)
        g.append(t)

    for k, v in state_action.items():
        g[k[0][0]][k[0][1]] = max(g[k[0][0]][k[0][1]], v[0])

    for i in range(n):
        for j in range(n):
            print("%-3.2f " % (g[i][j]), end="")
        print()


def print_heat_map(state_action, n):
    g = []
    for i in range(n):
        t = []
        for j in range(n):
            t.append(-9999.0)
        g.append(t)

    for k, v in state_action.items():
        g[k[0][0]][k[0][1]] = max(g[k[0][0]][k[0][1]], v[0])

    for i in range(n):
        for j in range(n):
            if(g[i][j] == -9999.0):
                g[i][j] = 0

    a = np.array(g)
    plt.imshow(a, cmap='hot', interpolation='nearest')
    plt.show()
import random
import gym
from log_util import *


class TD0_Cartpole:
    neg_inf = 0

    def __init__(self, env, exploration, learning_rate, discount_rate, scale):
        self.state_action = {}
        self.env = env
        self.exploration = exploration
        self.learning_rate = learning_rate
        self.discount_rate = discount_rate
        self.scale = scale

    def get_discrete_obs(self, observation):
        t = (observation * self.scale).astype(int)
        return tuple(t)

    def train(self, no_episodes, episode_length):
        avg_ep_len = 0
        for ep in range(no_episodes):
            obs = self.get_discrete_obs(self.env.reset())
            if ep % 500 == 0:
                print(ep, avg_ep_len/500)
                avg_ep_len = 0
            ep_len = 0
            for i in range(episode_length):
                action = self.get_action(obs)
                new_obs, reward, done, _ = self.env.step(action)
                new_obs = self.get_discrete_obs(new_obs)
                self.update_state_action(obs, new_obs, action, reward)
                obs = new_obs
                ep_len += 1
                if done:
                    break
            avg_ep_len += ep_len
            write_entry(ep, ep_len)
            self.exploration = self.exploration * 0.9995

    def set_state_action(self, sa):
        if sa not in self.state_action:
            self.state_action[sa] = self.neg_inf

    def get_action(self, obs):
        t = random.random()
        if t < self.exploration:
            return self.env.action_space.sample()
        else:
            act = self.env.action_space.sample()
            val = 0

            for action in range(self.env.action_space.n):
                sa = (obs, action)
                self.set_state_action(sa)
                if self.state_action[sa] > val:
                    act = action
                    val = self.state_action[sa]

            return act

    ## This is SARSA
    def update_state_action(self, obs, new_obs, action, reward):
        current_sa = (obs, action)
        self.set_state_action(current_sa)

        next_action = self.get_action(new_obs)
        next_sa = (new_obs, next_action)
        self.set_state_action(next_sa)

        self.state_action[current_sa] = self.state_action[current_sa] + self.learning_rate * \
            (reward + self.discount_rate *
             self.state_action[next_sa] - self.state_action[current_sa])

    def run(self, no_of_runs):
        for _ in range(no_of_runs):
            observation = self.get_discrete_obs(self.env.reset())

            while True:
                action = self.get_action(observation)
                new_observation, _, done, _ = self.env.step(action)
                new_observation = self.get_discrete_obs(new_observation)
                self.env.render()

                observation = new_observation

                if done:
                    break


def train():
    params = {
        'epsilon': 0.7,
        'learning_rate': 1,
        'decay': 1,
        'scale': 12
    }

    create_file("TD0_cartpole_exponential_decay", params)
    e = gym.make('CartPole-v1')
    tc = TD0_Cartpole(e, params['epsilon'], params['learning_rate'],
                      params['decay'], params['scale'])
    tc.train(10000, 502)
    save_file()
import random
from gridworld import *
from TD_util import *
from log_util import *


class TD0_Gridworld:
    neg_inf = -99

    def __init__(self, env, exploration, learning_rate, discount_rate):
        self.state_action = {}
        self.env = env
        self.exploration = exploration
        self.learning_rate = learning_rate
        self.discount_rate = discount_rate

    def train(self, no_episodes, episode_length):
        for ep in range(no_episodes):
            obs = self.env.reset()
            if obs in self.env.end_states:
                continue
            for i in range(episode_length):
                action = self.get_action(obs)
                new_obs, reward, done, _ = self.env.step(action)
                self.update_state_action(obs, new_obs, action, reward)
                obs = new_obs
                if done:
                    break
            self.exploration = self.exploration * 0.999

    def set_state_action(self, sa):
        if sa not in self.state_action:
            self.state_action[sa] = self.neg_inf

    def get_action(self, obs):
        t = random.random()
        if t < self.exploration:
            return self.env.get_random_action()
        else:
            act = self.env.get_random_action()
            val = self.neg_inf
            for action in self.env.get_actions():
                sa = (obs, action)
                self.set_state_action(sa)
                if self.state_action[sa] > val:
                    act = action
                    val = self.state_action[sa]

            return act

    ## This is SARSA
    def update_state_action(self, obs, new_obs, action, reward):
        current_sa = (obs, action)
        self.set_state_action(current_sa)

        next_action = self.get_action(new_obs)
        next_sa = (new_obs, next_action)
        self.set_state_action(next_sa)

        #  print("current sa = ", current_sa,
        #        "next sa = ", next_sa,
        #        "current sa val = ", self.state_action[current_sa],
        #        "next sa value = ", self.state_action[next_sa],
        #        "reward = ", reward,
        #        "to_update = ", reward + self.discount_rate *
        #        self.state_action[next_sa] - self.state_action[current_sa],
        #        "updated val = ", self.state_action[current_sa] + self.learning_rate * (reward + self.discount_rate * self.state_action[next_sa] - self.state_action[current_sa]))
        self.state_action[current_sa] = self.state_action[current_sa] + self.learning_rate * (
            reward + self.discount_rate * self.state_action[next_sa] - self.state_action[current_sa])


params = {
    'n': 15,
    'epsilon': 0.1,
    'learning_rate': 1,
    'decay': 1,
    'end_states': {(6, 9), (24, 29)}
}


def train():
    global params

    env = Gridworld(params['n'], 1, 1, params['end_states'], True)
    td = TD0_Gridworld(env, params['epsilon'], params['learning_rate'],
                       params['decay'])
    td.train(1000, 200)

    save_heat_map_td("TD0_Gridworld", params, td.state_action, params['n'])
import random
from TD_util import *
import gym
from model_util import *
from log_util import *


class TDN_Cartpole:
    neg_inf = 0
    retardation = 0.999

    def __init__(self, env, n, exploration, learning_rate, discount_rate, scale):
        self.state_action = {}
        self.env = env
        self.n = n
        self.exploration = exploration
        self.learning_rate = learning_rate
        self.discount_rate = discount_rate
        self.scale = scale

    def get_discrete_obs(self, observation):
        t = (observation * self.scale).astype(int)
        return tuple(t)

    def train(self, no_episodes, episode_length):
        global params
        avg_reward = 0
        for ep in range(1, no_episodes):
            states = []
            actions = []
            rewards = []
            #  self.new_states = 0
            #  self.old_states = 0

            obs = self.get_discrete_obs(self.env.reset())
            ep_reward = 0

            for i in range(episode_length):
                action = self.get_action(obs)
                new_obs, reward, done, _ = self.env.step(action)
                new_obs = self.get_discrete_obs(new_obs)

                states.append(obs)
                actions.append(action)
                rewards.append(reward)
                obs = new_obs
                ep_reward += reward
                if len(states) > self.n:
                    states = states[1:]
                    actions = actions[1:]
                    rewards = rewards[1:]

                if done:
                    while len(states) > 1:
                        self.update_state_action(states, actions, rewards)
                        states = states[1:]
                        actions = actions[1:]
                        rewards = rewards[1:]
                    break

                if len(states) < self.n:
                    continue

                self.update_state_action(states, actions, rewards)

            #  print("Ep over")
            avg_reward += ep_reward
            write_entry(ep, ep_reward)

            if ep % 500 == 0:
                print(ep, avg_reward/500)
                #  print("% new states = ", (self.new_states/(self.new_states +
                #                                             self.old_states)))
                #
                #  print("% old states = ", (self.old_states/(self.new_states +
                #                                             self.old_states)))
                avg_reward = 0

            if params['epi_decay']:
                self.exploration = self.exploration * self.retardation

    def set_state_action(self, sa):
        if sa not in self.state_action:
            self.state_action[sa] = 0

    def get_best_action(self, obs):
        act = self.env.action_space.sample()
        val = self.neg_inf
        for action in range(self.env.action_space.n):
            sa = (obs, action)
            self.set_state_action(sa)
            if self.state_action[sa] > val:
                act = action
                val = self.state_action[sa]

        return act

    def get_action(self, obs):
        t = random.random()
        if t < self.exploration:
            return self.env.action_space.sample()
        else:
            return self.get_best_action(obs)

    def run(self, episode_length):
        total_reward = 0
        obs = self.get_discrete_obs(self.env.reset())

        for i in range(episode_length):
            action = self.get_best_action(obs)
            new_obs, reward, done, _ = self.env.step(action)
            self.env.render()
            new_obs = self.get_discrete_obs(new_obs)
            total_reward += reward
            obs = new_obs

            if done:
                break
        return total_reward

    def update_state_action(self, states, actions, rewards):
        current_sa = (states[0], actions[0])
        self.set_state_action(current_sa)

        total_rewards = 0
        for i in range(len(rewards)):
            total_rewards += pow(self.discount_rate, i+1) * rewards[i]

        next_action = self.get_action(states[-1])
        next_sa = (states[-1], next_action)
        self.set_state_action(next_sa)

        self.state_action[current_sa] = self.state_action[current_sa] + self.learning_rate * (total_rewards + pow(
            self.discount_rate, self.n) * self.state_action[next_sa] - self.state_action[current_sa])


params = {
    'n': 20,
    'epsilon': 0.0001,
    'learning_rate': 0.8,
    'decay': 0.7,
    'scale': 13,
    'epi_decay': False,
    'load_model': True
}


def train():
    create_file("TD0_cartpole_exponential_decay", params)
    e = gym.make('CartPole-v0')

    td = TDN_Cartpole(e, params['n'], params['epsilon'], params['learning_rate'],
                      params['decay'], params['scale'])
    model_name = "saved-models/TDN-40-0.9-1-1-13.out"

    if params['load_model']:
        td.state_action = load_model(model_name)

    for i in range(3):
        td.train(10000, 500)
        save_model(td.state_action, model_name)
        print("Model saved")

    save_file()
import random
from gridworld import *
from TD_util import *
from log_util import *


class TDN_Gridworld:
    neg_inf = -99

    def __init__(self, env, n, exploration, learning_rate, discount_rate):
        self.state_action = {}
        self.env = env
        self.n = n
        self.exploration = exploration
        self.learning_rate = learning_rate
        self.discount_rate = discount_rate

    def train(self, no_episodes=10, episode_length=10):
        for ep in range(no_episodes):
            states = []
            actions = []
            rewards = []

            obs = self.env.reset()
            if obs in self.env.end_states:
                continue

            for i in range(episode_length):
                action = self.get_action(obs)
                new_obs, reward, done, _ = self.env.step(action)

                states.append(obs)
                actions.append(action)
                rewards.append(reward)
                obs = new_obs
                if len(states) > self.n:
                    states = states[1:]
                    actions = actions[1:]
                    rewards = rewards[1:]

                if done:
                    while len(states) > 1:
                        self.update_state_action(states, actions, rewards)
                        states = states[1:]
                        actions = actions[1:]
                        rewards = rewards[1:]
                    break

                if len(states) < self.n:
                    continue

                self.update_state_action(states, actions, rewards)

            #  print("Ep over")
            self.exploration = self.exploration * 0.999

    def set_state_action(self, sa):
        if sa not in self.state_action:
            self.state_action[sa] = 0

    def get_best_action(self, obs):
        act = self.env.get_random_action_at_state(obs)
        val = self.neg_inf
        for action in self.env.get_actions_at_state(obs):
            sa = (obs, action)
            self.set_state_action(sa)
            if self.state_action[sa] > val:
                act = action
                val = self.state_action[sa]

        return act

    def get_action(self, obs):
        t = random.random()
        if t < self.exploration:
            return self.env.get_random_action_at_state(obs)
        else:
            return self.get_best_action(obs)

    def run(self, episode_length):
        total_reward = 0
        obs = self.env.reset()
        if obs in self.env.end_states:
            return

        for i in range(episode_length):
            print(obs)
            action = self.get_best_action(obs)
            new_obs, reward, done, _ = env.step(action)
            total_reward += reward
            obs = new_obs

            if done:
                break
        return total_reward

    def update_state_action(self, states, actions, rewards):
        current_sa = (states[0], actions[0])
        self.set_state_action(current_sa)

        total_rewards = 0
        for i in range(len(rewards)):
            total_rewards += pow(self.discount_rate, i+1) * rewards[i]

        next_action = self.get_action(states[-1])
        next_sa = (states[-1], next_action)
        self.set_state_action(next_sa)
        #  print("updating")
        #  print(current_sa, self.state_action[current_sa])

        self.state_action[current_sa] = self.state_action[current_sa] + self.learning_rate * (total_rewards + pow(
            self.discount_rate, self.n) * self.state_action[next_sa] - self.state_action[current_sa])

        #  print(current_sa, self.state_action[current_sa])


params = {
    'no_of_states': 30,
    'n': 4,
    'epsilon': 0.1,
    'learning_rate': 1,
    'decay': 1,
    'end_states': {(6, 9), (24, 29)}
}


def train():
    global params
    env = Gridworld(params['no_of_states'], 2, 2, params['end_states'], True)
    td = TDN_Gridworld(env, params['n'], params['epsilon'], params['learning_rate'],
                       params['decay'])
    td.train(10000, 90)

    save_heat_map_td("TDN_Gridworld", params, td.state_action, env.n)
import matplotlib.pyplot as plt
import numpy as np


def print_state_action(state_action, n):
    g = []
    for i in range(n):
        t = []
        for j in range(n):
            t.append(-9999.0)
        g.append(t)

    for i in range(n):
        for j in range(n):
            for k, v in state_action.items():
                if k[0] == (i, j):
                    g[i][j] = max(g[i][j], v)

    for i in range(n):
        for j in range(n):
            if(g[i][j] == -9999.0):
                g[i][j] = 0

    for i in range(n):
        for j in range(n):
            print("%-3.2f " % (g[i][j]), end="")
        print()


def print_heat_map(state_action, n):
    g = []
    for i in range(n):
        t = []
        for j in range(n):
            t.append(-9999.0)
        g.append(t)

    for i in range(n):
        for j in range(n):
            for k, v in state_action.items():
                if k[0] == (i, j):
                    g[i][j] = max(g[i][j], v)

    for i in range(n):
        for j in range(n):
            if(g[i][j] == -9999.0):
                g[i][j] = 0

    for i in range(n):
        for j in range(n):
            print("%-3.2f " % (g[i][j]), end="")
        print()

    a = np.array(g)
    plt.imshow(a, cmap='hot', interpolation='nearest')
    plt.show()
