{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# (state,action) : [value,count]\n",
    "state_action = {}\n",
    "env = gym.make('CartPole-v1')\n",
    "scale = 12\n",
    "explore = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discrete_obs(observation):\n",
    "    t = (observation* scale).astype(int)\n",
    "    return tuple(t)\n",
    "\n",
    "def get_action(observation):\n",
    "    t = random.random()\n",
    "\n",
    "    action = env.action_space.sample()\n",
    "    best_score = 0\n",
    "    for k,v in state_action.items():\n",
    "        if k[0] == observation and v[0] > best_score:\n",
    "            action = k[1]\n",
    "            best_score = v[0]\n",
    "\n",
    "    if best_score > 0 and best_score/explore > t:\n",
    "        return action\n",
    "    else:\n",
    "        return env.action_space.sample()\n",
    "    \n",
    "\n",
    "def get_episode():\n",
    "    ## (state,action,reward)\n",
    "    episode = []\n",
    "    observation = env.reset()\n",
    "    observation = get_discrete_obs(observation)\n",
    "    \n",
    "    while True:\n",
    "        action = get_action(observation)\n",
    "        new_observation, reward, done, _ = env.step(action)\n",
    "        new_observation = get_discrete_obs(new_observation)\n",
    "        episode.append([observation,action,reward])\n",
    "        observation = new_observation\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    return episode\n",
    "\n",
    "def eval_episode(episode):\n",
    "    episode.reverse()\n",
    "    reward_so_far = 0\n",
    "    \n",
    "    for state,action,reward in episode:\n",
    "#         print(state,action,reward)\n",
    "        reward_so_far = reward_so_far + reward\n",
    "        \n",
    "        if (state,action) in state_action:\n",
    "            value,count = state_action[(state,action)]\n",
    "            new_val = (value * count + reward_so_far) / (count + 1)\n",
    "            state_action[(state,action)] = [new_val,count + 1]\n",
    "        \n",
    "        else:\n",
    "            state_action[(state,action)] = [reward_so_far,1]\n",
    "            \n",
    "\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train():\n",
    "    global exploration\n",
    "    avg_len = 0\n",
    "    max_len = 100\n",
    "    for i in range(1,5000):\n",
    "        ep = get_episode()\n",
    "        eval_episode(ep)\n",
    "        avg_len = avg_len + len(ep)\n",
    "        \n",
    "        if i%max_len == 0:\n",
    "            print(\"Avg is\",avg_len/max_len)\n",
    "            avg_len = 0\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg is 48.1\n",
      "Avg is 62.5\n",
      "Avg is 66.83\n",
      "Avg is 72.26\n",
      "Avg is 80.79\n",
      "Avg is 85.38\n",
      "Avg is 86.3\n",
      "Avg is 90.56\n",
      "Avg is 89.3\n",
      "Avg is 93.33\n",
      "Avg is 101.76\n",
      "Avg is 97.67\n",
      "Avg is 104.91\n",
      "Avg is 100.07\n",
      "Avg is 100.09\n",
      "Avg is 100.51\n",
      "Avg is 100.46\n",
      "Avg is 99.73\n",
      "Avg is 101.2\n",
      "Avg is 109.97\n",
      "Avg is 112.98\n",
      "Avg is 102.29\n",
      "Avg is 121.43\n",
      "Avg is 115.72\n",
      "Avg is 115.53\n",
      "Avg is 105.62\n",
      "Avg is 107.78\n",
      "Avg is 119.94\n",
      "Avg is 108.93\n",
      "Avg is 113.16\n",
      "Avg is 111.09\n",
      "Avg is 116.47\n",
      "Avg is 118.25\n",
      "Avg is 125.26\n",
      "Avg is 117.33\n",
      "Avg is 112.43\n",
      "Avg is 119.91\n",
      "Avg is 122.12\n",
      "Avg is 116.72\n",
      "Avg is 120.9\n",
      "Avg is 123.87\n",
      "Avg is 118.95\n",
      "Avg is 119.86\n",
      "Avg is 122.78\n",
      "Avg is 126.74\n",
      "Avg is 125.28\n",
      "Avg is 120.04\n",
      "Avg is 121.8\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
