{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEG_INF = -99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env:\n",
    "    index_to_direction = [\n",
    "        [ 0,-1],\n",
    "        [ 0, 1],\n",
    "        [-1, 0],\n",
    "        [ 1, 0]]\n",
    "    \n",
    "    def __init__(self, n = 5,start_x = 3,start_y = 3,end_states = {(0,0)},random_start = False):\n",
    "        self.start_x = start_x\n",
    "        self.start_y = start_y\n",
    "        self.x = self.start_x\n",
    "        self.y = self.start_y\n",
    "        self.n = n\n",
    "        self.end_states = end_states\n",
    "        self.random_start = random_start\n",
    "        \n",
    "    def get_next_state(self,action):\n",
    "        tx = self.x + self.index_to_direction[action][0]\n",
    "        ty = self.y + self.index_to_direction[action][1]\n",
    "            \n",
    "        return (tx,ty)\n",
    "    \n",
    "    def is_valid(self,x,y):\n",
    "        if x < 0 or x >= self.n or y < 0 or y >= self.n:\n",
    "            return False\n",
    "        return True\n",
    "        \n",
    "    def reset(self):\n",
    "        if self.random_start:\n",
    "            self.x = random.randint(0,self.n-1)\n",
    "            self.y = random.randint(0,self.n-1)\n",
    "        else:\n",
    "            self.x = self.start_x\n",
    "            self.y = self.start_y\n",
    "        return (self.x,self.y)\n",
    "    \n",
    "    def get_actions(self):\n",
    "        actions = []\n",
    "        for i in range(len(self.index_to_direction)):\n",
    "            if self.is_valid(self.x + self.index_to_direction[i][0],self.y + self.index_to_direction[i][1]):\n",
    "                actions.append(i)\n",
    "        return actions\n",
    "        \n",
    "    def get_random_action(self):\n",
    "        return random.choice(self.get_actions())\n",
    "    \n",
    "    def step(self,action):\n",
    "        self.x = self.x + self.index_to_direction[action][0]\n",
    "        self.y = self.y + self.index_to_direction[action][1]\n",
    "        \n",
    "        reward = -1\n",
    "        done = False\n",
    "        if (self.x,self.y) in self.end_states:\n",
    "            reward = 0\n",
    "            done = True\n",
    "            \n",
    "        return (self.x,self.y),reward,done,None\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_state_action():\n",
    "    g = []\n",
    "    for i in range(env.n):\n",
    "        t = []\n",
    "        for j in range(env.n):\n",
    "            t.append(-9999.0)\n",
    "        g.append(t)\n",
    "\n",
    "        \n",
    "    \n",
    "    for i in range(env.n):\n",
    "        for j in range(env.n):\n",
    "            for action in env.get_actions():\n",
    "                sa = ((i,j),action)\n",
    "                if (sa) in state_action:\n",
    "                    g[i][j] = max(g[i][j],state_action[sa])\n",
    "    \n",
    "    \n",
    "    for i in range(env.n):\n",
    "        for j in range(env.n):\n",
    "            if(g[i][j] == -9999.0):\n",
    "                g[i][j] = 0\n",
    "    \n",
    "    for i in range(env.n):\n",
    "        for j in range(env.n):\n",
    "            print(\"%-3.2f \"%(g[i][j]),end=\"\")\n",
    "        print()\n",
    "\n",
    "\n",
    "        \n",
    "def print_heat_map():\n",
    "    g = []\n",
    "    for i in range(env.n):\n",
    "        t = []\n",
    "        for j in range(env.n):\n",
    "            t.append(-9999.0)\n",
    "        g.append(t)\n",
    "\n",
    "        \n",
    "    \n",
    "    for i in range(env.n):\n",
    "        for j in range(env.n):\n",
    "            for action in env.get_actions():\n",
    "                sa = ((i,j),action)\n",
    "                if (sa) in state_action:\n",
    "                    g[i][j] = max(g[i][j],state_action[sa])\n",
    "    \n",
    "    \n",
    "    for i in range(env.n):\n",
    "        for j in range(env.n):\n",
    "            if(g[i][j] == -9999.0):\n",
    "                g[i][j] = 0\n",
    "    \n",
    "    for i in range(env.n):\n",
    "        for j in range(env.n):\n",
    "            print(\"%-3.2f \"%(g[i][j]),end=\"\")\n",
    "        print()\n",
    "\n",
    "    \n",
    "    a = np.array(g)\n",
    "    plt.imshow(a, cmap='hot', interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def set_state_action(sa):\n",
    "    global state_action\n",
    "    if sa not in state_action:\n",
    "        state_action[sa] = NEG_INF\n",
    "\n",
    "def get_action(obs):\n",
    "    global env\n",
    "    \n",
    "    t = random.random()\n",
    "    if t < exploration:\n",
    "        return env.get_random_action()\n",
    "    else:\n",
    "        act = env.get_random_action()\n",
    "        val = NEG_INF\n",
    "        for action in env.get_actions(): \n",
    "            sa = (obs,action)\n",
    "            set_state_action(sa)\n",
    "            if state_action[sa] > val:\n",
    "                act = action\n",
    "                val = state_action[sa]\n",
    "            \n",
    "        return act\n",
    "\n",
    "# def update_state_action(obs,new_obs,action,reward):\n",
    "#     current_sa = (obs,action)\n",
    "#     best_next_sa = (new_obs,action)\n",
    "#     set_state_action(best_next_sa)\n",
    "#     best_next_sa_val = NEG_INF\n",
    "    \n",
    "#     for act in env.get_actions():\n",
    "#         next_sa = (new_obs,act)\n",
    "#         set_state_action(next_sa)\n",
    "        \n",
    "#         if state_action[next_sa] > best_next_sa_val:\n",
    "#             best_next_sa_val = state_action[next_sa]\n",
    "#             best_next_sa = next_sa\n",
    "            \n",
    "#     state_action[current_sa] = state_action[current_sa] + learning_rate*(reward + discount_rate*state_action[best_next_sa] - state_action[current_sa]) \n",
    "\n",
    "\n",
    "## This is SARSA\n",
    "def update_state_action(obs,new_obs,action,reward):\n",
    "    current_sa = (obs,action)\n",
    "    set_state_action(current_sa)\n",
    "    \n",
    "    next_action = get_action(new_obs)\n",
    "    next_sa = (new_obs,next_action)\n",
    "    set_state_action(next_sa)\n",
    "            \n",
    "    state_action[current_sa] = state_action[current_sa] + learning_rate*(reward + discount_rate*state_action[next_sa] - state_action[current_sa]) \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(no_episodes = 10,episode_length = 10):\n",
    "    global exploration,env\n",
    "\n",
    "    for ep in range(no_episodes):\n",
    "        obs = env.reset()\n",
    "        for i in range(episode_length):\n",
    "            action = get_action(obs)\n",
    "            new_obs, reward, done, _ = env.step(action)\n",
    "            update_state_action(obs,new_obs,action,reward)\n",
    "            obs = new_obs\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        exploration = exploration * 0.98\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (state,action) -> reward\n",
    "state_action = {}\n",
    "\n",
    "exploration = 0.7\n",
    "learning_rate = 1\n",
    "discount_rate = 1\n",
    "env = Env(start_x = 3,start_y = 3,end_states = {(0,0)},random_start=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train(200,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-105.00 -107.00 -136.00 -135.00 -138.00 \n",
      "-116.00 -108.00 -135.00 -134.00 -112.00 \n",
      "-117.00 -107.00 -138.00 -137.00 -139.00 \n",
      "-131.00 -132.00 -131.00 -139.00 -139.00 \n",
      "-134.00 -131.00 -139.00 -138.00 -145.00 \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_heat_map()\n",
    "# print_state_action()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.012311562624005049"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
